{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ab1f067",
   "metadata": {},
   "source": [
    "# DiaTrend Forecasting Framework con Explainability\n",
    "\n",
    "Framework completo per il forecasting della glicemia con meccanismi di explainability.\n",
    "\n",
    "## Fasi del Framework:\n",
    "1. **Caricamento e Preprocessing Dati**\n",
    "2. **Costruzione e Training del Modello**\n",
    "3. **Explainability con SHAP**\n",
    "4. **Valutazione e Analisi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de0a792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST CREAZIONE MODELLI ===\n",
      "\n",
      "1. Test stacked_RNNs...\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "✅ LSTM Model creato con successo!\n",
      "   Input shape: (None, 12, 1)\n",
      "   Output shape: (None, 6)\n",
      "   Parametri: 50,310\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 12, 1)]           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 12, 64)            16896     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 12, 64)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,310\n",
      "Trainable params: 50,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "✅ LSTM Model creato con successo!\n",
      "   Input shape: (None, 12, 1)\n",
      "   Output shape: (None, 6)\n",
      "   Parametri: 50,310\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 12, 1)]           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 12, 64)            16896     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 12, 64)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,310\n",
      "Trainable params: 50,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "2. Test TransformerForecaster...\n",
      "\n",
      "2. Test TransformerForecaster...\n",
      "✅ TransformerForecaster creato con successo!\n",
      "   Input shape: (None, 12, 1)\n",
      "   Output shape: (None, 6)\n",
      "   Parametri: 198,438\n",
      "Model: \"transformer_forecaster\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " forecast_encoder (ForecastE  multiple                 191744    \n",
      " ncoder)                                                         \n",
      "                                                                 \n",
      " dense_5 (Dense)             multiple                  64        \n",
      "                                                                 \n",
      " forecast_head (Sequential)  (None, 6)                 6630      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,438\n",
      "Trainable params: 198,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "=== TEST COMPLETATO ===\n",
      "✅ TransformerForecaster creato con successo!\n",
      "   Input shape: (None, 12, 1)\n",
      "   Output shape: (None, 6)\n",
      "   Parametri: 198,438\n",
      "Model: \"transformer_forecaster\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " forecast_encoder (ForecastE  multiple                 191744    \n",
      " ncoder)                                                         \n",
      "                                                                 \n",
      " dense_5 (Dense)             multiple                  64        \n",
      "                                                                 \n",
      " forecast_head (Sequential)  (None, 6)                 6630      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,438\n",
      "Trainable params: 198,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "=== TEST COMPLETATO ===\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from stacked_RNNs import make_stacked_RNNs\n",
    "from ForecastEncoder import TransformerForecaster\n",
    "\n",
    "# Parametri comuni\n",
    "WINDOW_SIZE = 12      # Timesteps input\n",
    "FEATURES = 1          # Solo glucose  \n",
    "HORIZON = 6           # Timesteps da predire\n",
    "\n",
    "print(\"=== TEST CREAZIONE MODELLI ===\")\n",
    "\n",
    "# 1. Test stacked RNN\n",
    "print(\"\\n1. Test stacked_RNNs...\")\n",
    "try:\n",
    "    lstm_model = make_stacked_RNNs(\n",
    "        input_shape=(WINDOW_SIZE, FEATURES),  # (12, 1)\n",
    "        forecast_horizon=HORIZON,             # 6\n",
    "        dropout=0.1,\n",
    "        type_model='LSTM',\n",
    "        num_layers=2,\n",
    "        hidden_units=64,\n",
    "        bidirectional=False\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ LSTM Model creato con successo!\")\n",
    "    print(f\"   Input shape: {lstm_model.input_shape}\")\n",
    "    print(f\"   Output shape: {lstm_model.output_shape}\")\n",
    "    print(f\"   Parametri: {lstm_model.count_params():,}\")\n",
    "\n",
    "    lstm_model.summary()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Errore LSTM: {e}\")\n",
    "\n",
    "# 2. Test TransformerForecaster\n",
    "print(\"\\n2. Test TransformerForecaster...\")\n",
    "try:\n",
    "    transformer_model = TransformerForecaster(\n",
    "        num_layers=2,              # Numero layer encoder\n",
    "        d_model=64,                # Dimensione embedding\n",
    "        num_heads=4,               # Attention heads\n",
    "        dff=128,                   # Feed forward dim\n",
    "        input_features=FEATURES,   # 1 (glucose)\n",
    "        forecast_horizon=HORIZON,  # 6\n",
    "        dropout_rate=0.1,\n",
    "        aggregation='attention'    # Aggregazione learnable\n",
    "    )\n",
    "    \n",
    "    # Build del modello\n",
    "    transformer_model.build(input_shape=(None, WINDOW_SIZE, FEATURES))\n",
    "    \n",
    "    print(f\"✅ TransformerForecaster creato con successo!\")\n",
    "    print(f\"   Input shape: {(None, WINDOW_SIZE, FEATURES)}\")\n",
    "    print(f\"   Output shape: {(None, HORIZON)}\")\n",
    "    print(f\"   Parametri: {transformer_model.count_params():,}\")\n",
    "    \n",
    "    transformer_model.summary()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Errore Transformer: {e}\")\n",
    "\n",
    "print(\"\\n=== TEST COMPLETATO ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ecc68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Trasformers import Transformer\n",
    "transformer = Transformer(\n",
    "    num_layers=3,              # Numero di layer encoder/decoder\n",
    "    d_model=64,                # Dimensione embedding\n",
    "    num_heads=4,               # Numero attention heads\n",
    "    dff=256,                   # Dimensione feed forward\n",
    "    target_vocab_size=1,       # Per output numerico (forecasting)\n",
    "    use_timeseries_embedding=True,  # Attiva CNN 1D per time series\n",
    "    input_features=1,          # Numero feature input (solo glucose)\n",
    "    num_conv_layers=2,         # Numero layer CNN 1D\n",
    "    kernel_size=3,             # Dimensione kernel CNN\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "# Costruisci il modello specificando le dimensioni di input\n",
    "seq_len = 12  # Lunghezza sequenza (come WINDOW_SIZE)\n",
    "target_len = 6  # Lunghezza target (come HORIZON)\n",
    "\n",
    "# Build con le dimensioni corrette\n",
    "transformer.build([\n",
    "    (None, seq_len, 1),      # Input encoder: (batch, seq_len, features)\n",
    "    (None, target_len)       # Input decoder: (batch, target_len)\n",
    "])\n",
    "\n",
    "# Ora puoi chiamare summary\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf56f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test del nuovo ForecastEncoder ottimizzato\n",
    "from ForecastEncoder import TransformerForecaster, plot_attention_weights, plot_temporal_importance\n",
    "\n",
    "# Parametri per il modello ottimizzato\n",
    "forecaster = TransformerForecaster(\n",
    "    num_layers=3,              # Numero di layer encoder\n",
    "    d_model=64,                # Dimensione embedding\n",
    "    num_heads=8,               # Numero attention heads (aumentato per migliore rappresentazione)\n",
    "    dff=256,                   # Dimensione feed forward\n",
    "    input_features=1,          # Numero feature input (solo glucose)\n",
    "    forecast_horizon=6,        # Orizzonte di predizione (HORIZON)\n",
    "    dropout_rate=0.1,\n",
    "    num_conv_layers=2,         # CNN layers per embedding\n",
    "    kernel_size=3,\n",
    "    aggregation='attention',   # Aggregazione con attention learnable\n",
    "    max_seq_length=128         # Lunghezza massima sequenza\n",
    ")\n",
    "\n",
    "# Build con le dimensioni corrette\n",
    "seq_len = 12  # WINDOW_SIZE\n",
    "forecaster.build(input_shape=(None, seq_len, 1))\n",
    "\n",
    "print(\"=== FORECASTER TRANSFORMER OTTIMIZZATO ===\")\n",
    "forecaster.summary()\n",
    "\n",
    "# Compila il modello\n",
    "forecaster.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.01),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(f\"\\nParametri totali: {forecaster.count_params():,}\")\n",
    "print(f\"Orizzonte forecasting: {forecaster.forecast_horizon} timesteps\")\n",
    "print(f\"Metodo aggregazione: {forecaster.aggregation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa198aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test interpretabilità del ForecastEncoder\n",
    "print(\"\\n=== TEST INTERPRETABILITA' ===\")\n",
    "\n",
    "# Crea dati di esempio per test\n",
    "test_batch_size = 2\n",
    "test_seq_len = 12\n",
    "test_features = 1\n",
    "\n",
    "# Simula dati di glicemia\n",
    "np.random.seed(42)\n",
    "test_data = np.random.normal(120, 30, (test_batch_size, test_seq_len, test_features))\n",
    "test_data = np.clip(test_data, 70, 300)  # Range realistico glicemia\n",
    "test_tensor = tf.constant(test_data, dtype=tf.float32)\n",
    "\n",
    "print(f\"Dati test shape: {test_tensor.shape}\")\n",
    "print(f\"Range glicemia test: {test_data.min():.1f} - {test_data.max():.1f} mg/dL\")\n",
    "\n",
    "# Test predizione\n",
    "prediction = forecaster(test_tensor, training=False)\n",
    "print(f\"\\nPrediction shape: {prediction.shape}\")\n",
    "print(f\"Predizioni esempio: {prediction[0].numpy()}\")\n",
    "\n",
    "# Test estrazione attention maps\n",
    "print(\"\\nEstraendo attention maps...\")\n",
    "attention_maps = forecaster.get_attention_maps(test_tensor)\n",
    "\n",
    "print(\"\\nAttention maps disponibili:\")\n",
    "for key, value in attention_maps.items():\n",
    "    if isinstance(value, list):\n",
    "        print(f\"- {key}: {len(value)} layers\")\n",
    "        for i, layer_attention in enumerate(value):\n",
    "            if layer_attention is not None:\n",
    "                print(f\"  Layer {i}: {layer_attention.shape}\")\n",
    "    else:\n",
    "        print(f\"- {key}: {value.shape if value is not None else 'None'}\")\n",
    "\n",
    "# Test interpretazione completa\n",
    "print(\"\\nGenerando interpretazione completa...\")\n",
    "interpretation = forecaster.interpret_prediction(test_tensor)\n",
    "\n",
    "print(\"\\nInterpretazione disponibile:\")\n",
    "for key, value in interpretation.items():\n",
    "    if value is not None:\n",
    "        shape_info = value.shape if hasattr(value, 'shape') else type(value)\n",
    "        print(f\"- {key}: {shape_info}\")\n",
    "    else:\n",
    "        print(f\"- {key}: None\")\n",
    "\n",
    "# Mostra importanza temporale se disponibile\n",
    "if interpretation['temporal_importance'] is not None:\n",
    "    temp_importance = interpretation['temporal_importance'][0].numpy()\n",
    "    print(f\"\\nImportanza temporale (primo campione):\")\n",
    "    for i, importance in enumerate(temp_importance):\n",
    "        print(f\"  Timestep {i+1}: {importance:.4f}\")\n",
    "    print(f\"\\nTimestep più importante: {np.argmax(temp_importance) + 1}\")\n",
    "    print(f\"Importanza max: {np.max(temp_importance):.4f}\")\n",
    "\n",
    "print(\"\\n✅ Test interpretabilità completato con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import delle librerie necessarie\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import dei moduli custom\n",
    "from funcs import *\n",
    "from stacked_RNNs import *\n",
    "from explainability import *\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU disponibile: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Configurazione\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01bc72",
   "metadata": {},
   "source": [
    "## Fase 1: Caricamento e Preprocessing Dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acf8523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri del framework\n",
    "DATA_DIR = r'C:\\Users\\rosar\\Desktop\\UNISA\\Magistrale - Informatica\\II semestre\\DL\\timeseries\\dataset'\n",
    "DEMOGRAPHICS_FILE = r'C:\\Users\\rosar\\Desktop\\UNISA\\Magistrale - Informatica\\II semestre\\DL\\timeseries\\dataset\\SubjectDemographics_3-15-23.xlsx'\n",
    "\n",
    "# Parametri del modello\n",
    "WINDOW_SIZE = 12  # 12 letture passate (1 ora se ogni 5 min)\n",
    "HORIZON = 6       # 6 timesteps nel futuro (30 min se ogni 5 min)\n",
    "CATEGORICAL_COLS = ['Gender', 'Race']\n",
    "NUMERICAL_COLS = ['Age', 'Hemoglobin A1C']\n",
    "\n",
    "print(\"=== FASE 1: CARICAMENTO DATI ===\")\n",
    "\n",
    "# 1.1 Caricamento dati CGM\n",
    "print(\"Caricamento dati CGM...\")\n",
    "cgm_data = load_cgm_data(DATA_DIR)\n",
    "print(f\"Caricati dati per {len(cgm_data)} pazienti\")\n",
    "\n",
    "# Verifica formato dati CGM\n",
    "first_patient = list(cgm_data.keys())[0]\n",
    "print(f\"\\nFormato dati paziente {first_patient}:\")\n",
    "print(cgm_data[first_patient].head())\n",
    "print(f\"Colonne: {cgm_data[first_patient].columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321a9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Caricamento feature statiche\n",
    "print(\"\\nCaricamento feature statiche...\")\n",
    "static_features = load_static_features_from_excel(DEMOGRAPHICS_FILE)\n",
    "print(f\"Feature statiche caricate per {len(static_features)} pazienti\")\n",
    "print(\"\\nPrime righe feature statiche:\")\n",
    "print(static_features.head())\n",
    "print(f\"\\nColonne: {static_features.columns.tolist()}\")\n",
    "\n",
    "# Verifica valori mancanti\n",
    "print(\"\\nValori mancanti nelle feature statiche:\")\n",
    "print(static_features.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c840950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Preprocessing base e standardizzazione nomi colonne\n",
    "print(\"\\n=== PREPROCESSING BASE ===\")\n",
    "\n",
    "# Standardizza nomi colonne CGM se necessario\n",
    "standardized_cgm_data = {}\n",
    "for patient_id, df in cgm_data.items():\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Standardizza nomi colonne\n",
    "    if 'mg/dl' in df_copy.columns:\n",
    "        df_copy.rename(columns={'mg/dl': 'glucose'}, inplace=True)\n",
    "    elif 'mg/dL' in df_copy.columns:\n",
    "        df_copy.rename(columns={'mg/dL': 'glucose'}, inplace=True)\n",
    "    \n",
    "    # Converti date in datetime\n",
    "    if 'date' in df_copy.columns:\n",
    "        df_copy['date'] = pd.to_datetime(df_copy['date'])\n",
    "    \n",
    "    standardized_cgm_data[patient_id] = df_copy\n",
    "\n",
    "# Verifica dopo standardizzazione\n",
    "first_patient = list(standardized_cgm_data.keys())[0]\n",
    "print(f\"Colonne standardizzate: {standardized_cgm_data[first_patient].columns.tolist()}\")\n",
    "\n",
    "# Statistiche preliminari\n",
    "all_glucose_values = []\n",
    "for patient_id, df in standardized_cgm_data.items():\n",
    "    if 'glucose' in df.columns:\n",
    "        all_glucose_values.extend(df['glucose'].dropna().tolist())\n",
    "\n",
    "print(f\"\\nStatistiche globali glicemia:\")\n",
    "print(f\"- Numero totale letture: {len(all_glucose_values):,}\")\n",
    "print(f\"- Media: {np.mean(all_glucose_values):.2f} mg/dL\")\n",
    "print(f\"- Std: {np.std(all_glucose_values):.2f} mg/dL\")\n",
    "print(f\"- Min: {np.min(all_glucose_values):.2f} mg/dL\")\n",
    "print(f\"- Max: {np.max(all_glucose_values):.2f} mg/dL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928de55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Split pazienti in train/val/test\n",
    "print(\"\\n=== SPLIT PAZIENTI ===\")\n",
    "\n",
    "# Lista pazienti validi (con dati CGM e feature statiche)\n",
    "valid_patients = []\n",
    "for patient_id in standardized_cgm_data.keys():\n",
    "    if patient_id in static_features['SubjectID'].values:\n",
    "        df = standardized_cgm_data[patient_id]\n",
    "        if 'glucose' in df.columns and len(df['glucose'].dropna()) >= WINDOW_SIZE + HORIZON:\n",
    "            valid_patients.append(patient_id)\n",
    "\n",
    "print(f\"Pazienti validi per training: {len(valid_patients)}\")\n",
    "\n",
    "# Split cross-paziente\n",
    "train_patients, val_patients, test_patients = split_patients(\n",
    "    valid_patients, val_ratio=0.15, test_ratio=0.15, seed=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_patients)} pazienti\")\n",
    "print(f\"Validation: {len(val_patients)} pazienti\")\n",
    "print(f\"Test: {len(test_patients)} pazienti\")\n",
    "\n",
    "print(f\"\\nPazienti train: {train_patients[:5]}...\")  # Mostra primi 5\n",
    "print(f\"Pazienti val: {val_patients}\")\n",
    "print(f\"Pazienti test: {test_patients}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f45c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Preparazione dataset finale\n",
    "print(\"\\n=== PREPARAZIONE DATASET FINALE ===\")\n",
    "\n",
    "# Prepara dataset con finestre temporali, encoding feature statiche, scaling\n",
    "dataset = prepare_dataset_for_training(\n",
    "    cgm_data_dict=standardized_cgm_data,\n",
    "    static_features_df=static_features,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    horizon=HORIZON,\n",
    "    train_patients=train_patients,\n",
    "    val_patients=val_patients,\n",
    "    test_patients=test_patients,\n",
    "    categorical_cols=CATEGORICAL_COLS,\n",
    "    numerical_cols=NUMERICAL_COLS,\n",
    "    scaler_type='zscore'\n",
    ")\n",
    "\n",
    "# Verifica dimensioni dataset\n",
    "print(\"\\nDimensioni dataset:\")\n",
    "for split in ['train', 'val', 'test']:\n",
    "    if dataset[split]['X_seq'] is not None:\n",
    "        print(f\"{split.upper()}:\")\n",
    "        print(f\"  - X_seq: {dataset[split]['X_seq'].shape}\")\n",
    "        print(f\"  - X_static: {dataset[split]['X_static'].shape}\")\n",
    "        print(f\"  - y: {dataset[split]['y'].shape}\")\n",
    "    else:\n",
    "        print(f\"{split.upper()}: Nessun dato\")\n",
    "\n",
    "# Nomi delle feature statiche (per explainability)\n",
    "feature_names = CATEGORICAL_COLS + NUMERICAL_COLS\n",
    "static_dim = len(feature_names)\n",
    "print(f\"\\nFeature statiche ({static_dim}): {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501bfc3",
   "metadata": {},
   "source": [
    "## Fase 2: Costruzione e Training del Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== FASE 2: COSTRUZIONE MODELLO ===\")\n",
    "\n",
    "# Parametri modello\n",
    "SEQ_LEN = WINDOW_SIZE\n",
    "STATIC_DIM = static_dim\n",
    "LSTM_HIDDEN = 64\n",
    "FC_HIDDEN = 32\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "# Costruisci modello LSTM+Static\n",
    "model = build_lstm_static_model(\n",
    "    seq_len=SEQ_LEN,\n",
    "    static_dim=STATIC_DIM,\n",
    "    lstm_hidden=LSTM_HIDDEN,\n",
    "    fc_hidden=FC_HIDDEN,\n",
    "    num_layers=NUM_LAYERS\n",
    ")\n",
    "\n",
    "# Compila modello\n",
    "model = compile_model(model, learning_rate=0.001, loss='mse')\n",
    "\n",
    "# Visualizza architettura\n",
    "print(\"\\nArchitettura del modello:\")\n",
    "model.summary()\n",
    "\n",
    "# Plot del modello (opzionale)\n",
    "try:\n",
    "    tf.keras.utils.plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "    print(\"Diagramma architettura salvato in 'model_architecture.png'\")\n",
    "except:\n",
    "    print(\"Impossibile creare diagramma architettura (pydot non installato)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Training del modello\n",
    "print(\"\\n=== TRAINING MODELLO ===\")\n",
    "\n",
    "# Parametri training\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 15\n",
    "\n",
    "# Training\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    X_train_seq=dataset['train']['X_seq'],\n",
    "    X_train_static=dataset['train']['X_static'],\n",
    "    y_train=dataset['train']['y'],\n",
    "    X_val_seq=dataset['val']['X_seq'],\n",
    "    X_val_static=dataset['val']['X_static'],\n",
    "    y_val=dataset['val']['y'],\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    patience=PATIENCE\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completato!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaffb1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Visualizzazione learning curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss durante Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('MAE durante Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "lr_values = []\n",
    "for callback in model.history.history.get('lr', []):\n",
    "    lr_values.append(callback)\n",
    "if lr_values:\n",
    "    plt.plot(lr_values)\n",
    "    plt.title('Learning Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Learning Rate\\nnon disponibile', \n",
    "             ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiche finali training\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "best_val_loss = min(history.history['val_loss'])\n",
    "best_epoch = np.argmin(history.history['val_loss']) + 1\n",
    "\n",
    "print(f\"\\nStatistiche Training:\")\n",
    "print(f\"- Epoch totali: {len(history.history['loss'])}\")\n",
    "print(f\"- Miglior epoca: {best_epoch}\")\n",
    "print(f\"- Miglior val loss: {best_val_loss:.6f}\")\n",
    "print(f\"- Loss finale train: {final_train_loss:.6f}\")\n",
    "print(f\"- Loss finale val: {final_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0312df3",
   "metadata": {},
   "source": [
    "## Fase 3: Valutazione del Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== FASE 3: VALUTAZIONE MODELLO ===\")\n",
    "\n",
    "# 3.1 Valutazione su test set\n",
    "results = evaluate_model(\n",
    "    model=model,\n",
    "    X_test_seq=dataset['test']['X_seq'],\n",
    "    X_test_static=dataset['test']['X_static'],\n",
    "    y_test=dataset['test']['y']\n",
    ")\n",
    "\n",
    "print(f\"\\nMetriche Test Set (dati scalati):\")\n",
    "print(f\"- MSE: {results['mse']:.6f}\")\n",
    "print(f\"- MAE: {results['mae']:.6f}\")\n",
    "print(f\"- RMSE: {results['rmse']:.6f}\")\n",
    "\n",
    "# 3.2 Conversione a valori originali\n",
    "y_scaler = dataset['scalers']['y_scaler']\n",
    "\n",
    "# Converti predizioni e target ai valori originali\n",
    "y_test_original = inverse_transform_predictions(dataset['test']['y'], y_scaler)\n",
    "y_pred_original = inverse_transform_predictions(results['predictions'], y_scaler)\n",
    "\n",
    "# Metriche su valori originali\n",
    "mse_original = mean_squared_error(y_test_original, y_pred_original)\n",
    "mae_original = mean_absolute_error(y_test_original, y_pred_original)\n",
    "rmse_original = np.sqrt(mse_original)\n",
    "\n",
    "print(f\"\\nMetriche Test Set (valori originali mg/dL):\")\n",
    "print(f\"- MSE: {mse_original:.2f}\")\n",
    "print(f\"- MAE: {mae_original:.2f} mg/dL\")\n",
    "print(f\"- RMSE: {rmse_original:.2f} mg/dL\")\n",
    "\n",
    "# 3.3 Metriche cliniche\n",
    "clinical_metrics = calculate_clinical_metrics(y_test_original, y_pred_original)\n",
    "print(f\"\\nMetriche Cliniche:\")\n",
    "print(f\"- Accuracy range normale: {clinical_metrics['normal_accuracy']:.3f}\")\n",
    "print(f\"- Accuracy ipoglicemia: {clinical_metrics['hypoglycemia_accuracy']:.3f}\")\n",
    "print(f\"- Accuracy iperglicemia: {clinical_metrics['hyperglycemia_accuracy']:.3f}\")\n",
    "print(f\"- MAPE: {clinical_metrics['mean_absolute_percentage_error']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed6140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Visualizzazione risultati\n",
    "print(\"\\n=== VISUALIZZAZIONE RISULTATI ===\")\n",
    "\n",
    "# Plot predizioni vs valori reali\n",
    "plot_prediction_vs_actual(y_test_original, y_pred_original, \n",
    "                          title=\"Predizioni vs Valori Reali - Test Set\")\n",
    "\n",
    "# Distribuzione errori\n",
    "errors = y_pred_original - y_test_original\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Distribuzione Errori Predizione')\n",
    "plt.xlabel('Errore (mg/dL)')\n",
    "plt.ylabel('Frequenza')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.boxplot([errors])\n",
    "plt.title('Boxplot Errori')\n",
    "plt.ylabel('Errore (mg/dL)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(y_test_original, errors, alpha=0.6)\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Errori vs Valori Reali')\n",
    "plt.xlabel('Valori Reali (mg/dL)')\n",
    "plt.ylabel('Errore (mg/dL)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStatistiche Errori:\")\n",
    "print(f\"- Errore medio: {np.mean(errors):.2f} mg/dL\")\n",
    "print(f\"- Std errori: {np.std(errors):.2f} mg/dL\")\n",
    "print(f\"- Errore mediano: {np.median(errors):.2f} mg/dL\")\n",
    "print(f\"- 95% errori entro: ±{np.percentile(np.abs(errors), 95):.2f} mg/dL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b3e294",
   "metadata": {},
   "source": [
    "## Fase 4: Explainability con SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55231fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== FASE 4: EXPLAINABILITY ===\")\n",
    "\n",
    "# 4.1 Inizializzazione framework XAI\n",
    "xai_framework = XAIFramework(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "print(\"Framework XAI inizializzato\")\n",
    "print(f\"Feature statiche: {feature_names}\")\n",
    "\n",
    "# 4.2 Setup SHAP explainer\n",
    "print(\"\\nConfigurazione SHAP explainer...\")\n",
    "xai_framework.setup_shap_explainer(\n",
    "    background_size=50,  # Ridotto per velocità\n",
    "    explainer_type='gradient'  # Più veloce di 'deep'\n",
    ")\n",
    "\n",
    "# 4.3 Calcolo valori SHAP\n",
    "print(\"\\nCalcolo valori SHAP...\")\n",
    "shap_values = xai_framework.calculate_shap_values(\n",
    "    max_samples=30  # Limitato per demo\n",
    ")\n",
    "\n",
    "print(\"Valori SHAP calcolati!\")\n",
    "print(f\"Shape SHAP sequenza: {shap_values[0].shape}\")\n",
    "print(f\"Shape SHAP features statiche: {shap_values[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e248b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Visualizzazione importanza globale\n",
    "print(\"\\n=== ANALISI IMPORTANZA GLOBALE ===\")\n",
    "\n",
    "xai_framework.plot_global_importance()\n",
    "\n",
    "# Summary testuale\n",
    "summary = xai_framework.generate_explanation_summary()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08050839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Analisi campioni specifici\n",
    "print(\"\\n=== ANALISI CAMPIONI SPECIFICI ===\")\n",
    "\n",
    "# Analizza 3 campioni diversi\n",
    "for i in range(min(3, len(xai_framework.test_indices))):\n",
    "    print(f\"\\n--- Campione {i} ---\")\n",
    "    xai_framework.plot_sample_explanation(sample_idx=i)\n",
    "    \n",
    "    # Spiegazione clinica\n",
    "    sample_seq = dataset['test']['X_seq'][xai_framework.test_indices[i]].reshape(1, -1, 1)\n",
    "    sample_static = dataset['test']['X_static'][xai_framework.test_indices[i]].reshape(1, -1)\n",
    "    \n",
    "    clinical_explanation = create_clinical_explanation(\n",
    "        model, sample_seq, sample_static, feature_names\n",
    "    )\n",
    "    print(\"\\nSpiegazione Clinica:\")\n",
    "    print(clinical_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1271f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 Analisi interazioni feature\n",
    "print(\"\\n=== ANALISI INTERAZIONI FEATURE ===\")\n",
    "\n",
    "xai_framework.analyze_feature_interactions()\n",
    "\n",
    "# Analisi correlazioni SHAP\n",
    "static_shap = shap_values[1]\n",
    "seq_shap = shap_values[0]\n",
    "\n",
    "# Correlazione tra importanza feature statiche e importanza sequenza\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, feat_name in enumerate(feature_names):\n",
    "    plt.subplot(2, 2, i+1 if i < 4 else 4)\n",
    "    \n",
    "    # Importanza feature statica per ogni campione\n",
    "    feat_importance = np.abs(static_shap[:, i])\n",
    "    \n",
    "    # Importanza media sequenza per ogni campione\n",
    "    seq_importance = np.mean(np.abs(seq_shap), axis=(1, 2))\n",
    "    \n",
    "    plt.scatter(feat_importance, seq_importance, alpha=0.6)\n",
    "    plt.xlabel(f'Importanza {feat_name}')\n",
    "    plt.ylabel('Importanza Media Sequenza')\n",
    "    plt.title(f'Correlazione {feat_name} vs Sequenza')\n",
    "    \n",
    "    # Calcola correlazione\n",
    "    if len(feat_importance) > 1:\n",
    "        corr = np.corrcoef(feat_importance, seq_importance)[0, 1]\n",
    "        plt.text(0.05, 0.95, f'r = {corr:.3f}', transform=plt.gca().transAxes,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5c18b",
   "metadata": {},
   "source": [
    "## Fase 5: Analisi per Paziente e Conclusioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6428185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== ANALISI PER PAZIENTE ===\")\n",
    "\n",
    "# Esempio di analisi per paziente specifico\n",
    "sample_patient = test_patients[0] if test_patients else None\n",
    "\n",
    "if sample_patient:\n",
    "    print(f\"\\nAnalisi dettagliata paziente: {sample_patient}\")\n",
    "    \n",
    "    # Dati del paziente\n",
    "    patient_cgm = standardized_cgm_data[sample_patient]\n",
    "    patient_static = static_features[static_features['SubjectID'] == sample_patient]\n",
    "    \n",
    "    print(f\"Dati CGM disponibili: {len(patient_cgm)} letture\")\n",
    "    print(f\"Range temporale: {patient_cgm['date'].min()} - {patient_cgm['date'].max()}\")\n",
    "    \n",
    "    if not patient_static.empty:\n",
    "        print(\"\\nCaratteristiche paziente:\")\n",
    "        for col in ['Age', 'Gender', 'Race', 'Hemoglobin A1C']:\n",
    "            if col in patient_static.columns:\n",
    "                value = patient_static[col].iloc[0]\n",
    "                print(f\"- {col}: {value}\")\n",
    "    \n",
    "    # Visualizza serie temporale del paziente\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(patient_cgm['date'], patient_cgm['glucose'], 'b-', linewidth=1, alpha=0.8)\n",
    "    plt.axhspan(70, 180, alpha=0.2, color='green', label='Range Normale')\n",
    "    plt.axhspan(0, 70, alpha=0.2, color='red', label='Ipoglicemia')\n",
    "    plt.axhspan(180, 400, alpha=0.2, color='orange', label='Iperglicemia')\n",
    "    plt.title(f'Serie Temporale Completa - {sample_patient}')\n",
    "    plt.ylabel('Glicemia (mg/dL)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    # Ultime 24 ore (assumendo 1 lettura ogni 5 min = 288 letture/giorno)\n",
    "    last_24h = patient_cgm.tail(288) if len(patient_cgm) > 288 else patient_cgm\n",
    "    plt.plot(last_24h['date'], last_24h['glucose'], 'b-', linewidth=2)\n",
    "    plt.axhspan(70, 180, alpha=0.2, color='green')\n",
    "    plt.axhspan(0, 70, alpha=0.2, color='red')\n",
    "    plt.axhspan(180, 400, alpha=0.2, color='orange')\n",
    "    plt.title('Ultime 24 ore (o dati disponibili)')\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('Glicemia (mg/dL)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiche del paziente\n",
    "    glucose_values = patient_cgm['glucose'].dropna()\n",
    "    print(f\"\\nStatistiche glicemiche {sample_patient}:\")\n",
    "    print(f\"- Media: {glucose_values.mean():.2f} mg/dL\")\n",
    "    print(f\"- Std: {glucose_values.std():.2f} mg/dL\")\n",
    "    print(f\"- TIR (70-180): {((glucose_values >= 70) & (glucose_values <= 180)).mean()*100:.1f}%\")\n",
    "    print(f\"- Tempo in ipoglicemia (<70): {(glucose_values < 70).mean()*100:.1f}%\")\n",
    "    print(f\"- Tempo in iperglicemia (>180): {(glucose_values > 180).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb1f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusioni e salvataggio modello\n",
    "print(\"\\n=== CONCLUSIONI ===\")\n",
    "\n",
    "print(f\"\"\"\n",
    "FRAMEWORK DIATREND FORECASTING - RISULTATI FINALI\n",
    "================================================\n",
    "\n",
    "DATASET:\n",
    "- Pazienti totali processati: {len(valid_patients)}\n",
    "- Finestre temporali create: {len(dataset['train']['y']) + len(dataset['val']['y']) + len(dataset['test']['y']):,}\n",
    "- Lunghezza finestra input: {WINDOW_SIZE} timesteps\n",
    "- Orizzonte predizione: {HORIZON} timesteps\n",
    "\n",
    "MODELLO:\n",
    "- Architettura: LSTM + Feature Statiche\n",
    "- Feature statiche utilizzate: {len(feature_names)}\n",
    "- Parametri totali: {model.count_params():,}\n",
    "\n",
    "PERFORMANCE:\n",
    "- RMSE: {rmse_original:.2f} mg/dL\n",
    "- MAE: {mae_original:.2f} mg/dL\n",
    "- MAPE: {clinical_metrics['mean_absolute_percentage_error']:.2f}%\n",
    "- Accuracy range normale: {clinical_metrics['normal_accuracy']:.1%}\n",
    "\n",
    "EXPLAINABILITY:\n",
    "- Framework SHAP implementato\n",
    "- Analisi importanza feature statiche vs sequenze CGM\n",
    "- Spiegazioni a livello di campione e globale\n",
    "- Visualizzazioni clinicamente interpretabili\n",
    "\n",
    "Il framework è pronto per deployment e ulteriori analisi!\n",
    "\"\"\")\n",
    "\n",
    "# Salva modello\n",
    "model.save('diatrend_lstm_static_model.h5')\n",
    "print(\"\\nModello salvato in 'diatrend_lstm_static_model.h5'\")\n",
    "\n",
    "# Salva dataset e scalers\n",
    "import pickle\n",
    "with open('dataset_and_scalers.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'dataset': dataset,\n",
    "        'feature_names': feature_names,\n",
    "        'window_size': WINDOW_SIZE,\n",
    "        'horizon': HORIZON\n",
    "    }, f)\n",
    "print(\"Dataset e scalers salvati in 'dataset_and_scalers.pkl'\")\n",
    "\n",
    "print(\"\\n🎉 Framework DiaTrend completato con successo!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
